import os
import csv
import requests
import time
import re

# --- Configuration ---
# The CSV file generated by the Google Scholar script.
INPUT_CSV = "google_scholar_results_with_status.csv"
# The new CSV that will be created with the download status.
OUTPUT_CSV = "google_scholar_results_with_status.csv"
# The folder where PDFs will be saved.
OUTPUT_FOLDER = "phase1"
# Column names in your CSV file.
URL_COLUMN = 'url'
TITLE_COLUMN = 'title'

COPIED_COOKIE_STRING = r"_hjSessionUser_1290436=eyJpZCI6Ijk3ZGNkNGJjLTBkMGQtNTM1Zi1iMGM3LWVkZWI1MGYyNTcxNiIsImNyZWF0ZWQiOjE2NDk2NjUwODg0NzYsImV4aXN0aW5nIjp0cnVlfQ==; MAID=04egCXaDP7a9yyK5lyrbow==; CookieConsent={stamp:%27ty5c0yNZtEiC1T2lcX97PnViCmUKvkNFJOo/LwHkHSfkcF8rYhKtsw==%27%2Cnecessary:true%2Cpreferences:false%2Cstatistics:false%2Cmarketing:false%2Cmethod:%27explicit%27%2Cver:1%2Cutc:1729080912601%2Cregion:%27sg%27}; _parsely_visitor={%22id%22:%22pid=73e0acb6-7002-42fc-8e81-e112d216f9a8%22%2C%22session_count%22:1%2C%22last_session_ts%22:1737020553840}; _cfuvid=b8RKJIsgDmTUEBB3tJkkgGjF7i017v5tIp09zeoDTPk-1749541832614-0.0.1.1-604800000; _hp2_ses_props.1083010732=%7B%22r%22%3A%22https%3A%2F%2Fwww.google.com%2F%22%2C%22ts%22%3A1751873971265%2C%22d%22%3A%22dl.acm.org%22%2C%22h%22%3A%22%2Fdoi%2F10.1016%2Fj.jss.2023.111934%22%7D; __cf_bm=ErPKYjhX5qiNYW2eqEflVfLpJqALsSLclVDIUsDoV4Y-1751874000-1.0.1.1-xzCLnnNZnuJEM7wr82Fj5HsyRJZfwDUQrlddJS7VS7sudQU2PrQ6IXuI85jCUt1Lp2PcnrJd9J_.7FjbRWcdpgFMwSCPXFCdXESQZrzgl_U; MACHINE_LAST_SEEN=2025-07-07T00%3A40%3A03.054-07%3A00; JSESSIONID=6C6BB5235BB8DB740EFD671C957D4F8C; _hjSession_1290436=eyJpZCI6ImZiMzhmMzI4LWU2NDAtNDRlYi1iMzZjLTJhNmYzNWY1ZmJmYSIsImMiOjE3NTE4NzQwMTU2MjUsInMiOjAsInIiOjAsInNiIjowLCJzciI6MCwic2UiOjAsImZzIjowLCJzcCI6MX0=; cf_clearance=6NAVp62POpKbh2g4nBrcQ8Nu0aIc8.WgWaw1RNGEHGM-1751874747-1.2.1.1-YWcCeyms3M1I8y5KGdzcHIz1k5baGVV7uLTsEZ_qwLE8IiPOCPJkKeRMR8SIV9LFPTMNdt4vXUvZWWiLofHq5LMD95PAncXtq.GS4jLeIPEzJVwxf39mBOlPoq4OYbQ9fEPBFUzIuVKOpxDpNFM7s7Gdp6dq6NQHD0jab4Fc8fy5a7asgcisQsYER2V.FhKhsQdqRw0zWgDHjBC9D0f0_vUlLBDj85CV.GGmTmeifyU; _cfuvid=sjg4AhPWRbGJeSsZ0yUoJR5tjNEUhcANjRnTuENrpMo-1751874860597-0.0.1.1-604800000; _hp2_id.1083010732=%7B%22userId%22%3A%228055440840863630%22%2C%22pageviewId%22%3A%224924639315782799%22%2C%22sessionId%22%3A%222181753077435968%22%2C%22identity%22%3Anull%2C%22trackerVersion%22%3A%224.0%22%7D"

# This header will make your script look like your logged-in browser
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
    'Cookie': COPIED_COOKIE_STRING
}

def sanitize_filename(filename):
    """Cleans a string to be a valid filename."""
    sanitized = re.sub(r'[\\/*?:"<>|]', "", filename)
    sanitized = sanitized.replace(' ', '_')
    return sanitized[:150]

# --- Main Script ---

print("Starting PDF download process for Google Scholar results...")

# 1. Create the output folder if it doesn't exist
os.makedirs(OUTPUT_FOLDER, exist_ok=True)
print(f"üìÅ Output folder '{OUTPUT_FOLDER}' is ready.")

# 2. Check if the input CSV file exists
if not os.path.exists(INPUT_CSV):
    print(f"‚ùå Error: The file '{INPUT_CSV}' was not found.")
    exit()

# 3. Read the CSV data into memory
papers_data = []
with open(INPUT_CSV, 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    for row in reader:
        papers_data.append(row)
        

print(f"\nFound {len(papers_data)} papers in '{INPUT_CSV}' to process.")

# 4. Loop through papers, attempt download, and update status
for idx, paper in enumerate(papers_data, start=1):
    # Default status is 'failed'
    # if has key download_status, read it, otherwise, create as failed
    if not 'download_status' in paper: 
        paper['download_status'] = 'failed'
    if paper.get('download_status') == 'successful':
        print(f"[{idx}/{len(papers_data)}]‚úÖ SKIPPING: File already exists as marked in the csv.")
        continue
    # check if google_idx prefix file exists, if so, skip
    # no need to check file name after the prefix
    if os.path.exists(os.path.join(OUTPUT_FOLDER, f"google_{idx}.pdf")):
        print(f"[{idx}/{len(papers_data)}]‚úÖ SKIPPING: File already exists as found in the output folder.")
        paper['download_status'] = 'successful'
        continue
    title = paper.get(TITLE_COLUMN, f"untitled_paper_{idx}")
    url = paper.get(URL_COLUMN)

    print(f"\n[{idx}/{len(papers_data)}] Processing: {title}")

    if not url:
        print("   ‚ö†Ô∏è SKIPPING: No URL found.")
        continue
    
    # Create the indexed and sanitized filename
    safe_filename = f"google_{idx}_{sanitize_filename(title)}.pdf"
    output_path = os.path.join(OUTPUT_FOLDER, safe_filename)

    if os.path.exists(output_path):
        print(f"   ‚úÖ SKIPPING: File already exists.")
        paper['download_status'] = 'successful'
        continue

    try:
        print(f"   Attempting download from: {url}")
        # Use a header to mimic a browser request
        headers = HEADERS
        response = requests.get(url, headers=headers, stream=True, timeout=30, allow_redirects=True)
        response.raise_for_status()

        # IMPORTANT: Check if the content is actually a PDF
        content_type = response.headers.get('Content-Type', '').lower()
        if 'application/pdf' in content_type:
            # Save the PDF file
            with open(output_path, 'wb') as pdf_file:
                for chunk in response.iter_content(chunk_size=8192):
                    pdf_file.write(chunk)
            print(f"   ‚úÖ Success! Saved to '{output_path}'")
            paper['download_status'] = 'successful'
        else:
            print(f"   ‚ö†Ô∏è SKIPPING: Link is not a direct PDF (Content-Type: {content_type})")

    except requests.exceptions.RequestException as e:
        print(f"   ‚ùå FAILED to download. Error: {e}")

    # Be polite to servers
    time.sleep(2)

# 5. Write the updated data to a new CSV file
new_fieldnames = list(papers_data[0].keys())
try:
    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=new_fieldnames)
        writer.writeheader()
        writer.writerows(papers_data)
    print(f"\nüéâ Process complete! Status updated in '{OUTPUT_CSV}'.")
except Exception as e:
    print(f"\n‚ùå Error writing to status CSV file: {e}")